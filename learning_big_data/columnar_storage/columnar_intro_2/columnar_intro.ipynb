{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pure columnar format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#first_name:1,last_name:2,department:3,salary:4\n",
    "#\n",
    "\n",
    "import os\n",
    "\n",
    "if os.path.exists('employees.col1'):\n",
    "    os.remove('employees.col1')\n",
    "\n",
    "with open('employees.col1', 'w') as f:\n",
    "    f.write('{\"columns\": {\"first_name\":{\"start\": 0, \"end\": 0},\"last_name\":{\"start\": 0, \"end\": 0},\"department\":{\"start\": 0, \"end\": 0},\"salary\":{\"start\": 0, \"end\": 0}}}\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "faker = Faker('en')\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_employee(salary):\n",
    "    first_name = faker.first_name()\n",
    "    last_name = faker.last_name()\n",
    "    department = faker.random_element(elements=['Embedded','BigData','HR'])\n",
    "    return {'first_name': first_name, 'last_name': last_name, 'department': department, 'salary': salary}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "salaries = np.random.normal(16000, 6000, 10)\n",
    "\n",
    "employees = []\n",
    "\n",
    "for salary in salaries:\n",
    "    employees.append(create_employee(int(salary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def decode_file(path):\n",
    "    with open(path, 'r') as f:\n",
    "        header = json.loads(f.readline())\n",
    "        end_of_meta = f.tell()\n",
    "        columns = []\n",
    "        for column_name in header['columns'].keys():\n",
    "            column = header['columns'][column_name]\n",
    "            f.seek(end_of_meta + column['start'])\n",
    "            data_string = f.read(column['end'] - column['start']) \n",
    "            data = data_string.split(',') if data_string else []\n",
    "            columnData = { 'name': column_name, 'data': data}\n",
    "            columns.append(columnData)\n",
    "    return (header, columns)\n",
    "\n",
    "\n",
    "def encode_file(path, columns):\n",
    "    new_header = {'columns': {}}\n",
    "    current_offset = 0\n",
    "    for column in columns:\n",
    "        joined_data_len = len(','.join(column['data']))\n",
    "        new_header['columns'][column['name']] = { \n",
    "            'start': current_offset, \n",
    "            'end': current_offset + joined_data_len\n",
    "        }\n",
    "        current_offset = current_offset + joined_data_len\n",
    "\n",
    "    with open(path, 'w') as f:\n",
    "            f.write(json.dumps(new_header) + '\\n')\n",
    "            for column in columns:\n",
    "                f.write(','.join(column['data']))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all(path):\n",
    "    header, columns = decode_file(path)\n",
    "\n",
    "    records = None\n",
    "    for column in columns:\n",
    "        for i, el in enumerate(column['data']):\n",
    "            if not records:\n",
    "                records = [None] * len(column['data'])\n",
    "            if not records[i]:\n",
    "                records[i] = {}\n",
    "            records[i][column['name']] = el\n",
    "        \n",
    "        \n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert(path, employee):\n",
    "    header, columns = decode_file(path)\n",
    "    for column in columns:\n",
    "        print(column['data'], column[\"name\"])\n",
    "        column['data'].append(f'{employee[column[\"name\"]]}')\n",
    "    encode_file(path, columns)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] first_name\n",
      "[] last_name\n",
      "[] department\n",
      "[] salary\n",
      "['James'] first_name\n",
      "['Meyer'] last_name\n",
      "['BigData'] department\n",
      "['15813'] salary\n",
      "['James', 'Molly'] first_name\n",
      "['Meyer', 'Pacheco'] last_name\n",
      "['BigData', 'HR'] department\n",
      "['15813', '11311'] salary\n",
      "['James', 'Molly', 'Amy'] first_name\n",
      "['Meyer', 'Pacheco', 'Moore'] last_name\n",
      "['BigData', 'HR', 'Embedded'] department\n",
      "['15813', '11311', '16454'] salary\n",
      "['James', 'Molly', 'Amy', 'Sandra'] first_name\n",
      "['Meyer', 'Pacheco', 'Moore', 'Baker'] last_name\n",
      "['BigData', 'HR', 'Embedded', 'HR'] department\n",
      "['15813', '11311', '16454', '14939'] salary\n",
      "['James', 'Molly', 'Amy', 'Sandra', 'Kathleen'] first_name\n",
      "['Meyer', 'Pacheco', 'Moore', 'Baker', 'Peterson'] last_name\n",
      "['BigData', 'HR', 'Embedded', 'HR', 'HR'] department\n",
      "['15813', '11311', '16454', '14939', '11764'] salary\n",
      "['James', 'Molly', 'Amy', 'Sandra', 'Kathleen', 'Rhonda'] first_name\n",
      "['Meyer', 'Pacheco', 'Moore', 'Baker', 'Peterson', 'Branch'] last_name\n",
      "['BigData', 'HR', 'Embedded', 'HR', 'HR', 'Embedded'] department\n",
      "['15813', '11311', '16454', '14939', '11764', '11218'] salary\n",
      "['James', 'Molly', 'Amy', 'Sandra', 'Kathleen', 'Rhonda', 'Katherine'] first_name\n",
      "['Meyer', 'Pacheco', 'Moore', 'Baker', 'Peterson', 'Branch', 'Strickland'] last_name\n",
      "['BigData', 'HR', 'Embedded', 'HR', 'HR', 'Embedded', 'BigData'] department\n",
      "['15813', '11311', '16454', '14939', '11764', '11218', '27736'] salary\n",
      "['James', 'Molly', 'Amy', 'Sandra', 'Kathleen', 'Rhonda', 'Katherine', 'Michael'] first_name\n",
      "['Meyer', 'Pacheco', 'Moore', 'Baker', 'Peterson', 'Branch', 'Strickland', 'Santiago'] last_name\n",
      "['BigData', 'HR', 'Embedded', 'HR', 'HR', 'Embedded', 'BigData', 'BigData'] department\n",
      "['15813', '11311', '16454', '14939', '11764', '11218', '27736', '19568'] salary\n",
      "['James', 'Molly', 'Amy', 'Sandra', 'Kathleen', 'Rhonda', 'Katherine', 'Michael', 'Richard'] first_name\n",
      "['Meyer', 'Pacheco', 'Moore', 'Baker', 'Peterson', 'Branch', 'Strickland', 'Santiago', 'Dawson'] last_name\n",
      "['BigData', 'HR', 'Embedded', 'HR', 'HR', 'Embedded', 'BigData', 'BigData', 'BigData'] department\n",
      "['15813', '11311', '16454', '14939', '11764', '11218', '27736', '19568', '4625'] salary\n"
     ]
    }
   ],
   "source": [
    "for e in employees:\n",
    "    insert('employees.col1', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_eq(path, column_name, value):    \n",
    "    header, columns = decode_file(path)\n",
    "    \n",
    "    searched_column = next(col for col in columns if col['name'] == column_name)\n",
    "    found_indices = [i for i, r in enumerate(searched_column['data']) if r == value]\n",
    "\n",
    "    for column in columns:\n",
    "        for index in found_indices:\n",
    "            del column['data'][index]\n",
    "            \n",
    "    print(columns)\n",
    "    encode_file(path, columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_eq(path, searched_column_name, searched_value, new_record):\n",
    "    header, columns = decode_file(path)\n",
    "\n",
    "    searched_column = next(col for col in columns if col['name'] == searched_column_name)\n",
    "    found_index = searched_column['data'].index(searched_value)\n",
    "\n",
    "    for column in columns:\n",
    "        column[index] = new_record[column['name']]\n",
    "    \n",
    "    encode_file(path, columns)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_eq_full_search(path, column_name, value, column_names):\n",
    "    header, columns = decode_file(path)\n",
    "    \n",
    "    searched_column = next(col for col in columns if col['name'] == column_name)\n",
    "    found_indices = [i for i, r in enumerate(searched_column['data']) if r == value]\n",
    "  \n",
    "    \n",
    "    records = []\n",
    "    for column in columns:\n",
    "        for i, found_index in enumerate(found_indices):\n",
    "            if len(records) <= i:\n",
    "                records.append({})            \n",
    "            if not column_names or len(column_names) == 0 or column['name'] in column_names:\n",
    "                records[i][column['name']] = column['data'][found_index]\n",
    "        \n",
    "    return records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_eq(path, column_name, value, select_columns): # column_names => selected_columns\n",
    "    with open(path, 'r') as f:\n",
    "        meta = json.loads(f.readline())\n",
    "        ref_offset = f.tell() # ref offset\n",
    "        \n",
    "        filter_column = meta['columns'][column_name]\n",
    "    \n",
    "        \n",
    "        # znajdz indeksy na których pojawia sie dana wartosc\n",
    "        \n",
    "        start, end = filter_column['start'], filter_column['end']\n",
    "        f.seek(ref_offset + start)\n",
    "        raw_values = f.read(end - start) \n",
    "        values = raw_values and raw_values.split(',') or []\n",
    "        found_indices = [i for i, r in enumerate(values) if r == value]\n",
    "                \n",
    "        data = []\n",
    "        \n",
    "        # excercise: majac dane liste indeksów - wyciagnij całe rekordy\n",
    "        \n",
    "        \n",
    "        # excercise: majac dane liste indeksów - wyciagnij liste kolumn\n",
    "        \n",
    "        for column in select_columns:\n",
    "            start, end = meta['columns'][column]['start'], meta['columns'][column]['end']\n",
    "            \n",
    "            f.seek(ref_offset + start)\n",
    "            raw_values = f.read(end - start) \n",
    "            values = raw_values and raw_values.split(',') or []\n",
    "            \n",
    "            data.append([values[index] for index in found_indices])\n",
    "        \n",
    "    display(select_columns, list(zip(*data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------+\n",
      "| first_name | last_name  | department |\n",
      "+============+============+============+\n",
      "| James      | Meyer      | BigData    |\n",
      "+------------+------------+------------+\n",
      "| Katherine  | Strickland | BigData    |\n",
      "+------------+------------+------------+\n",
      "| Michael    | Santiago   | BigData    |\n",
      "+------------+------------+------------+\n",
      "| Richard    | Dawson     | BigData    |\n",
      "+------------+------------+------------+\n"
     ]
    }
   ],
   "source": [
    "find_eq('employees.col1', 'department', 'BigData', ['first_name', 'last_name', 'department'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_gt(path, column_name, value, column_names):\n",
    "    header, columns = decode_file(path)\n",
    "    \n",
    "    with open(path, 'r') as f:\n",
    "        header = json.loads(f.readline())\n",
    "        end_of_meta = f.tell()\n",
    "        \n",
    "        filter_column = [col for col in header['columns'] if col['name'] == column_name][0]\n",
    "        \n",
    "        f.seek(end_of_meta + filter_column['start_offset'])\n",
    "        data_string = f.read(filter_column['end_offset'] - filter_column['start_offset']) \n",
    "        data = data_string.split(',') if data_string else []\n",
    "        searched_column = { 'name': filter_column['name'], 'data': data}\n",
    "        found_indices = [i for i, r in enumerate(searched_column['data']) if r > value]\n",
    "        \n",
    "        selected_columns = [col for col in header['columns'] if not column_names or len(column_names) == 0 or col['name'] in column_names]\n",
    "        columns = []\n",
    "        for column in selected_columns:\n",
    "            f.seek(end_of_meta + column['start_offset'])\n",
    "            data_string = f.read(column['end_offset'] - column['start_offset']) \n",
    "            data = data_string.split(',') if data_string else []\n",
    "            columnData = { 'name': column['name'], 'data': data}\n",
    "            columns.append(columnData)\n",
    "        \n",
    "    \n",
    "    records = []\n",
    "    for column in columns:\n",
    "        for i, found_index in enumerate(found_indices):\n",
    "            if len(records) <= i:\n",
    "                records.append({})            \n",
    "            records[i][column['name']] = column['data'][found_index]\n",
    "        \n",
    "        \n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lt(path, column_name, value, column_names):\n",
    "    header, columns = decode_file(path)\n",
    "    \n",
    "    with open(path, 'r') as f:\n",
    "        header = json.loads(f.readline())\n",
    "        end_of_meta = f.tell()\n",
    "        \n",
    "        filter_column = [col for col in header['columns'] if col['name'] == column_name][0]\n",
    "        \n",
    "        f.seek(end_of_meta + filter_column['start_offset'])\n",
    "        data_string = f.read(filter_column['end_offset'] - filter_column['start_offset']) \n",
    "        data = data_string.split(',') if data_string else []\n",
    "        searched_column = { 'name': filter_column['name'], 'data': data}\n",
    "        found_indices = [i for i, r in enumerate(searched_column['data']) if r < value]\n",
    "        \n",
    "        selected_columns = [col for col in header['columns'] if not column_names or len(column_names) == 0 or col['name'] in column_names]\n",
    "        columns = []\n",
    "        for column in selected_columns:\n",
    "            f.seek(end_of_meta + column['start_offset'])\n",
    "            data_string = f.read(column['end_offset'] - column['start_offset']) \n",
    "            data = data_string.split(',') if data_string else []\n",
    "            columnData = { 'name': column['name'], 'data': data}\n",
    "            columns.append(columnData)\n",
    "        \n",
    "    \n",
    "    records = []\n",
    "    for column in columns:\n",
    "        for i, found_index in enumerate(found_indices):\n",
    "            if len(records) <= i:\n",
    "                records.append({})            \n",
    "            records[i][column['name']] = column['data'][found_index]\n",
    "        \n",
    "        \n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'first_name': 'James',\n",
       "  'last_name': 'Meyer',\n",
       "  'department': 'BigData',\n",
       "  'salary': '15813'}]"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'employees.col1'\n",
    "\n",
    "find_eq(path, 'first_name', 'James', [])\n",
    "# find_all(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"columns\": {\"first_name\": {\"start\": 0, \"end\": 68}, \"last_name\": {\"start\": 68, \"end\": 145}, \"department\": {\"start\": 145, \"end\": 206}, \"salary\": {\"start\": 206, \"end\": 264}}}\r\n",
      "James,Molly,Amy,Sandra,Kathleen,Rhonda,Katherine,Michael,Richard,AmyMeyer,Pacheco,Moore,Baker,Peterson,Branch,Strickland,Santiago,Dawson,MartinezBigData,HR,Embedded,HR,HR,Embedded,BigData,BigData,BigData,HR15813,11311,16454,14939,11764,11218,27736,19568,4625,16704"
     ]
    }
   ],
   "source": [
    "!cat employees.col1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Format - przedstaw plik\n",
    "    a. ćwiczenie - funkcja zwracająca listę wartości danej kolumny (meta, f.seek(start), f.read(end-start))\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|  name  | age |\n",
      "+========+=====+\n",
      "| Maciek | 33  |\n",
      "+--------+-----+\n",
      "| Bartek | 28  |\n",
      "+--------+-----+\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from texttable import Texttable\n",
    "\n",
    "\n",
    "def display(headers, rows):\n",
    "    table = Texttable()\n",
    "    table.add_rows([headers] + rows)\n",
    "    print(table.draw())\n",
    "    \n",
    "\n",
    "\n",
    "selected_names = ['name', 'age']\n",
    "values = [\n",
    "    ['Maciek', 'Bartek'],\n",
    "    [33, 28],\n",
    "]\n",
    "\n",
    "# print(list(zip(values)))\n",
    "\n",
    "display(selected_names, list(zip(*values)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
